{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\ba}{\\boldsymbol{a}}\n",
    "\\newcommand{\\bb}{\\boldsymbol{b}}\n",
    "\\newcommand{\\bc}{\\boldsymbol{c}}\n",
    "\\newcommand{\\bd}{\\boldsymbol{d}}\n",
    "\\newcommand{\\be}{\\boldsymbol{e}}\n",
    "\\newcommand{\\bff}{\\boldsymbol{f}}\n",
    "\\newcommand{\\bg}{\\boldsymbol{g}}\n",
    "\\newcommand{\\bh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\bi}{\\boldsymbol{i}}\n",
    "\\newcommand{\\bj}{\\boldsymbol{j}}\n",
    "\\newcommand{\\bk}{\\boldsymbol{k}}\n",
    "\\newcommand{\\bl}{\\boldsymbol{l}}\n",
    "\\newcommand{\\bm}{\\boldsymbol{m}}\n",
    "\\newcommand{\\bn}{\\boldsymbol{n}}\n",
    "\\newcommand{\\bo}{\\boldsymbol{o}}\n",
    "\\newcommand{\\bp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\bq}{\\boldsymbol{q}}\n",
    "\\newcommand{\\br}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bs}{\\boldsymbol{s}}\n",
    "\\newcommand{\\bt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\bu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\bv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\bw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\bx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\by}{\\boldsymbol{y}}\n",
    "\\newcommand{\\bz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\bA}{\\boldsymbol{A}}\n",
    "\\newcommand{\\bB}{\\boldsymbol{B}}\n",
    "\\newcommand{\\bC}{\\boldsymbol{C}}\n",
    "\\newcommand{\\bD}{\\boldsymbol{D}}\n",
    "\\newcommand{\\bE}{\\boldsymbol{E}}\n",
    "\\newcommand{\\bF}{\\boldsymbol{F}}\n",
    "\\newcommand{\\bG}{\\boldsymbol{G}}\n",
    "\\newcommand{\\bH}{\\boldsymbol{H}}\n",
    "\\newcommand{\\bI}{\\boldsymbol{I}}\n",
    "\\newcommand{\\bJ}{\\boldsymbol{J}}\n",
    "\\newcommand{\\bK}{\\boldsymbol{K}}\n",
    "\\newcommand{\\bL}{\\boldsymbol{L}}\n",
    "\\newcommand{\\bM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\bN}{\\boldsymbol{N}}\n",
    "\\newcommand{\\bO}{\\boldsymbol{O}}\n",
    "\\newcommand{\\bP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\bQ}{\\boldsymbol{Q}}\n",
    "\\newcommand{\\bR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\bS}{\\boldsymbol{S}}\n",
    "\\newcommand{\\bT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\bU}{\\boldsymbol{U}}\n",
    "\\newcommand{\\bV}{\\boldsymbol{V}}\n",
    "\\newcommand{\\bW}{\\boldsymbol{W}}\n",
    "\\newcommand{\\bX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\bY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\bZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\btheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bXi}{\\boldsymbol{\\Xi}}\n",
    "\\newcommand{\\bPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bOmega}{\\boldsymbol{\\Omega}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonnegative Matrix Factorization\n",
    "##### Dr. Hua Zhou, Aug 10, 2016\n",
    "##### SAMSI Opt Program Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 0.4.6\n",
      "Commit 2e358ce (2016-06-19 17:16 UTC)\n",
      "Platform Info:\n",
      "  System: Darwin (x86_64-apple-darwin13.4.0)\n",
      "  CPU: Intel(R) Core(TM) i7-3720QM CPU @ 2.60GHz\n",
      "  WORD_SIZE: 64\n",
      "  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Sandybridge)\n",
      "  LAPACK: libopenblas64_\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-3.3\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that NNMF finds a low rank approximation to the data matrix $\\bX$ by minimizing\n",
    "$$\n",
    "    \\|\\bX - \\bV \\bW\\|_{\\text{F}}^2.\n",
    "$$\n",
    "The MM algorithm iterate according to\n",
    "$$\n",
    "    \\bV \\gets \\bV \\, .\\times \\, (\\bX \\bW^T) \\, ./ \\, (\\bV \\bW \\bW^T)\n",
    "$$\n",
    "$$\n",
    "    \\bW \\gets \\bW \\, .\\times \\, (\\bV^T \\bX) \\, ./ \\, (\\bV^T \\bV \\bW)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume data files or symbolic links to data files are in the current folder. First load in the face data set and display a couple sample faces. Note each row is a face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = readdlm(\"nnmf-2429-by-361-face.txt\", ' ', Float64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the starting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V0full = readdlm(\"V0.txt\", ' ', Float64)\n",
    "W0full = readdlm(\"W0.txt\", ' ', Float64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `nnmf.jl` implements the multiplicative algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnmf (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# This function fits rank-r nonnegative matrix factorization (NNMF) of\n",
    "# a data matrix X with nonnegative entries by a multiplicative algorithm.\n",
    "#\n",
    "function nnmf(X::Matrix{Float64}, r::Int;\n",
    "              maxiter::Int=1000, tolfun::Float64=1e-4,\n",
    "              V::Matrix{Float64} = rand(size(X, 1), r),\n",
    "              W::Matrix{Float64} = rand(r, size(X, 2)),\n",
    "              device::Symbol = :CPU, \n",
    "              precision::Symbol = :DP)\n",
    "  # size of data matrix\n",
    "  m = size(X, 1)\n",
    "  n = size(X, 2)\n",
    "  mn = m * n\n",
    "\n",
    "  if device == :CPU\n",
    "\n",
    "    # MM loop on CPU\n",
    "    Vnum = zeros(V)\n",
    "    Vden = zeros(V)\n",
    "    Wnum = zeros(W)\n",
    "    Wden = zeros(W)\n",
    "    B = V * W\n",
    "    XminusB = X - B\n",
    "    obj = vecnorm(XminusB)^2\n",
    "    for iter = 1:maxiter\n",
    "\n",
    "      # multiplicative update of V and W\n",
    "      # V = V .* (X * W') ./ (B * W')\n",
    "      BLAS.gemm!('N', 'T', 1.0, X, W, 0.0, Vnum)\n",
    "      BLAS.gemm!('N', 'T', 1.0, B, W, 0.0, Vden)\n",
    "      for j = 1:r\n",
    "        @simd for i = 1:m\n",
    "          @inbounds V[i, j] *= (Vnum[i, j] / Vden[i, j])\n",
    "        end\n",
    "      end\n",
    "      # B = V * W\n",
    "      BLAS.gemm!('N', 'N', 1.0, V, W, 0.0, B)\n",
    "      # W = W .* (V' * X) ./ (V' * B)\n",
    "      BLAS.gemm!('T', 'N', 1.0, V, X, 0.0, Wnum)\n",
    "      BLAS.gemm!('T', 'N', 1.0, V, B, 0.0, Wden)\n",
    "      for j = 1:n\n",
    "        @simd for i = 1:r\n",
    "          @inbounds W[i, j] *= (Wnum[i, j] / Wden[i, j])\n",
    "        end\n",
    "      end\n",
    "      # B = V * W\n",
    "      BLAS.gemm!('N', 'N', 1.0, V, W, 0.0, B)\n",
    "\n",
    "      # check convergence\n",
    "      objold = obj\n",
    "      BLAS.blascopy!(mn, X, 1, XminusB, 1)\n",
    "      BLAS.axpy!(mn, -1.0, B, 1, XminusB, 1)\n",
    "      obj = vecnorm(XminusB); obj = obj * obj\n",
    "      if abs(obj - objold) < tolfun * (abs(obj) + 1.0)\n",
    "        break\n",
    "      end\n",
    "    end\n",
    "\n",
    "    # output\n",
    "    return V, W\n",
    "\n",
    "  elseif device == :GPU\n",
    "    # MM loop on CPU\n",
    "\n",
    "    # transfer data X, V, W to GPU\n",
    "    # and load kernel function\n",
    "    md = CUDArt.CuModule(\"vecop.ptx\", false)\n",
    "    if precision == :DP\n",
    "      d_X = CUDArt.CudaArray(X)\n",
    "      d_V = CUDArt.CudaArray(V)\n",
    "      d_W = CUDArt.CudaArray(W)\n",
    "      vmuldiv = CUDArt.CuFunction(md, \"vmuldiv_dp\")\n",
    "    elseif precision == :SP\n",
    "      d_X = CUDArt.CudaArray(map(Float32, X))\n",
    "      d_V = CUDArt.CudaArray(map(Float32, V))\n",
    "      d_W = CUDArt.CudaArray(map(Float32, W))\n",
    "      vmuldiv = CUDArt.CuFunction(md, \"vmuldiv_sp\")\n",
    "    else\n",
    "      error(\"unrecognized precision: SP or DP\")\n",
    "    end\n",
    "    # constants 1 and 0\n",
    "    oneconst = one(eltype(d_X))\n",
    "    zeroconst = zero(eltype(d_X))\n",
    "\n",
    "    # pre-allocate variables on GPU\n",
    "    d_Vnum = CUDArt.CudaArray(eltype(d_V), size(d_V))\n",
    "    d_Vden = CUDArt.CudaArray(eltype(d_V), size(d_V))\n",
    "    d_Wnum = CUDArt.CudaArray(eltype(d_W), size(d_W))\n",
    "    d_Wden = CUDArt.CudaArray(eltype(d_W), size(d_W))\n",
    "    d_B = CUDArt.CudaArray(eltype(d_X), size(d_X))\n",
    "    d_XminusB = CUDArt.CudaArray(eltype(d_X), size(d_X))\n",
    "\n",
    "    # initial objective value\n",
    "    CUBLAS.gemm!('N', 'N', oneconst, d_V, d_W, zeroconst, d_B)\n",
    "    CUBLAS.blascopy!(mn, d_X, 1, d_XminusB, 1)\n",
    "    CUBLAS.axpy!(mn, -oneconst, d_B, 1, d_XminusB, 1)\n",
    "    obj = CUBLAS.nrm2(mn, d_XminusB, 1); obj = obj * obj\n",
    "\n",
    "    for iter = 1:maxiter\n",
    "\n",
    "      # V = V .* (X * W') ./ (B * W')\n",
    "      CUBLAS.gemm!('N', 'T', oneconst, d_X, d_W, zeroconst, d_Vnum)\n",
    "      CUBLAS.gemm!('N', 'T', oneconst, d_B, d_W, zeroconst, d_Vden)\n",
    "      CUDArt.launch(vmuldiv, m, r, (d_Vnum, d_Vden, d_V))\n",
    "      # B = V * W\n",
    "      CUBLAS.gemm!('N', 'N', oneconst, d_V, d_W, zeroconst, d_B)\n",
    "      # W = W .* (V' * X) ./ (V' * B)\n",
    "      CUBLAS.gemm!('T', 'N', oneconst, d_V, d_X, zeroconst, d_Wnum)\n",
    "      CUBLAS.gemm!('T', 'N', oneconst, d_V, d_B, zeroconst, d_Wden)\n",
    "      CUDArt.launch(vmuldiv, r, n, (d_Wnum, d_Wden, d_W))\n",
    "      # B = V * W\n",
    "      CUBLAS.gemm!('N', 'N', oneconst, d_V, d_W, zeroconst, d_B)\n",
    "\n",
    "      # check convergence\n",
    "      objold = obj\n",
    "      CUBLAS.blascopy!(mn, d_X, 1, d_XminusB, 1)\n",
    "      CUBLAS.axpy!(mn, -oneconst, d_B, 1, d_XminusB, 1)\n",
    "      obj = CUBLAS.nrm2(mn, d_XminusB, 1); obj = obj * obj\n",
    "      if abs(obj - objold) < tolfun * (abs(obj) + 1.0)\n",
    "        break\n",
    "      end\n",
    "\n",
    "    end\n",
    "\n",
    "    # transfer result to host and output\n",
    "    V = CUDArt.to_host(d_V)\n",
    "    W = CUDArt.to_host(d_W)\n",
    "    return V, W\n",
    "\n",
    "  elseif device == \"cvx\"\n",
    "\n",
    "    for iter = 1:maxiter\n",
    "      if mod(iter, 2) == 1\n",
    "        # update V\n",
    "        V = Convex.Variable(m, r)\n",
    "        problem = Convex.minimize(vecnorm(X - V * W, 2))\n",
    "        problem.constraints += V >= 0\n",
    "      else\n",
    "        # update W\n",
    "        W = Convex.Variable(r, n)\n",
    "        problem = Convex.minimize(vecnorm(X - V * W, 2))\n",
    "        problem.constraints += W >= 0\n",
    "      end\n",
    "      Convex.solve!(problem)\n",
    "    end\n",
    "\n",
    "  else\n",
    "\n",
    "    error(\"unrecognized device: CPU or GPU\")\n",
    "\n",
    "  end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug and profile CPU code using r = 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.718640 seconds (937.55 k allocations: 59.388 MB, 0.17% gc time)\n",
      "6621.627345486279\n"
     ]
    }
   ],
   "source": [
    "r = 30\n",
    "V0 = V0full[:, 1:r]\n",
    "W0 = W0full[1:r, :]\n",
    "gc()\n",
    "#Profile.clear()\n",
    "#profile V, W = nnmf(X, r; V=V0, W=W0)\n",
    "#Profile.print(format=:flat)\n",
    "@time V, W = nnmf(X, r; V=copy(V0), W=copy(W0));\n",
    "println(vecnorm(X - V * W)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug and profile GPU code using r = 30. Before that make sure we use NVIDIA's `nvcc` compiler to compile our kernel function `vecop.cu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extern \"C\"\n",
      "{\n",
      "\n",
      "__global__ void vmuldiv_dp(const double *a, const double *b, double *c)\n",
      "{\n",
      "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "  c[idx] *= a[idx] / b[idx];\n",
      "}\n",
      "\n",
      "__global__ void vmuldiv_sp(const float *a, const float *b, float *c)\n",
      "{\n",
      "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "  c[idx] *= a[idx] / b[idx];\n",
      "}\n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "run(`cat vecop.cu`)\n",
    "run(`nvcc -ptx -gencode=arch=compute_20,code=sm_20 vecop.cu`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.730161 seconds (457.46 k allocations: 24.510 MB, 0.10% gc time)\n",
      "6621.627018329714\n",
      "  8.901734 seconds (204.69 k allocations: 9.660 MB)\n",
      "6621.627345486279\n"
     ]
    }
   ],
   "source": [
    "using CUDArt, CUBLAS\n",
    "@time V, W = nnmf(X, r; V=copy(V0), W=copy(W0), device=:GPU, precision=:SP);\n",
    "println(vecnorm(X - V * W)^2)\n",
    "@time V, W = nnmf(X, r; V=copy(V0), W=copy(W0), device=:GPU, precision=:DP);\n",
    "println(vecnorm(X - V * W)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's time our algorithm at ranks r = 10, 20, 30, 40 and 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank = 10\n",
      "rank = 20\n",
      "rank = 30\n",
      "rank = 40\n",
      "rank = 50\n"
     ]
    }
   ],
   "source": [
    "rlist = 10:10:50\n",
    "runtime = zeros(length(rlist), 3)\n",
    "objval = zeros(length(rlist), 3)\n",
    "for i = 1:length(rlist)\n",
    "    # rank\n",
    "    r = rlist[i]\n",
    "    println(\"rank = $r\")\n",
    "    # starting value\n",
    "    V0 = V0full[:, 1:r]\n",
    "    W0 = W0full[1:r, :]\n",
    "    # CPU version\n",
    "    runtime[i, 1] = @elapsed V, W = nnmf(X, r; V=copy(V0), W=copy(W0))\n",
    "    objval[i, 1] = vecnorm(X - V * W)^2\n",
    "    # GPU-SP version\n",
    "    runtime[i, 2] = @elapsed V, W = nnmf(X, r; V=copy(V0), W=copy(W0), \n",
    "        device=:GPU, precision=:SP)\n",
    "    objval[i, 2] = vecnorm(X - V * W)^2\n",
    "    # GPU-DP version\n",
    "    runtime[i, 3] = @elapsed V, W = nnmf(X, r; V=copy(V0), W=copy(W0), \n",
    "        device=:GPU, precision=:DP)\n",
    "    objval[i, 3] = vecnorm(X - V * W)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x6 Array{Float64,2}:\n",
       "  3.22638  1.30994   3.20499  11.7304   11.7304   11.7304 \n",
       "  4.86586  2.85439   6.60161   8.49722   8.49722   8.49722\n",
       "  7.169    4.40029   8.56611   6.62163   6.62163   6.62163\n",
       "  8.39851  5.24118  14.741     5.25666   5.25666   5.25666\n",
       " 11.7062   3.9505   21.4917    4.4302    4.4302    4.4302 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[runtime objval/1e3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
